<!-- â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
     â”‚     ğŸ“Š Statistics Â· Problem 1 â€” Exploring the CLT        â”‚
     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ -->

# ğŸ“Š **Problem 1: Exploring the Central Limit Theorem through Simulations**

> *â€œRegardless of the original distribution, the mean of many samples whispers â€˜normal.â€™â€*  

---

## ğŸ“‹ Table of Contents
1. [Motivation ğŸ¯](#motivation)  
2. [Theoretical Background ğŸ”­](#theory)  
3. [Simulation Design ğŸ”](#design)  
4. [Sampling & Visualization ğŸ“ˆ](#visualization)  
5. [Parameter Exploration âš™ï¸](#parameters)  
6. [Python Implementation ğŸ’»](#implementation)  
7. [Model Limitations & Extensions ğŸ”§](#limitations)  
8. [Results & Interpretation ğŸ§©](#interpretation)  
9. [Conclusions ğŸ“](#conclusions)  
10. [Further Reading ğŸ“š](#references)  

---

<a name="motivation"></a>
## 1 Â· Motivation ğŸ¯  
The **Central Limit Theorem** (CLT) underpins statistical inference:  
> *The distribution of the sample mean \(\bar X\) tends toward normality as sample size \(n\) grows, regardless of the parent distribution.*  
Simulations make this abstract theorem tangible, showing how quickly â€œnormalâ€ emerges.

---

<a name="theory"></a>
## 2 Â· Theoretical Background ğŸ”­  

- For i.i.d. random variables \(X_1,\dots,X_n\) with mean \(\mu\) and variance \(\sigma^2\):  
  $$
  \bar X = \frac{1}{n}\sum_{i=1}^n X_i,
  $$
  $$
  \frac{\bar X - \mu}{\sigma/\sqrt{n}}
    \;\xrightarrow{d}\;
    \mathcal{N}(0,1)
    \quad\text{as }n\to\infty.
  $$

- **Implication:** even a highly skewed or discrete population yields an approximately Gaussian \(\bar X\) for moderate \(n\).  

---

<a name="design"></a>
## 3 Â· Simulation Design ğŸ”  
1. **Populations:**  
   - Uniform on [0,1]  
   - Exponential with rate Î»=1  
   - Binomial(n=10, p=0.3)  
2. **Population size:** generate 100 000 draws per distribution.  
3. **Sample sizes:** \(n \in \{5, 10, 30, 50\}\).  
4. **Repetitions:** for each \(n\), repeat sampling 2000 times and record \(\bar X\).

---

<a name="visualization"></a>
## 4 Â· Sampling & Visualization ğŸ“ˆ  
For each population and each \(n\):  
- Compute the 2000 sample means.  
- Plot their histogram overlaid with the theoretical normal PDF  
  \(\mathcal{N}(\mu,\,\sigma^2/n)\).

---

<a name="parameters"></a>
## 5 Â· Parameter Exploration âš™ï¸  
- **Effect of \(n\):** larger \(n\) â†’ tighter, more symmetric histograms.  
- **Effect of variance:** high-variance populations (exponential) converge more slowly.  
- **Skewness:** binomial (discrete, bounded) shows faster â€œnormalizationâ€ than exponential.

---

<a name="implementation"></a>
## 6 Â· Python Implementation ğŸ’»

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1qtuoE05v44tcGhbwzsDE3kAASEYv_lUU?usp=sharing)

<img src="https://i.imgur.com/Nec5baF.gif" width="1100">

<a name="interpretation"></a>

---

<a name="limitations"></a>
## 7 Â· Model Limitations & Extensions ğŸ”§

### ğŸ” Limitations
- **Finite sample effects:**  
  The CLT is asymptotic â€” for small \(n\), sampling distributions may still deviate noticeably from normality, especially with skewed or heavy-tailed populations.

- **Independence assumption:**  
  The classical CLT assumes i.i.d. samples. Correlated data (e.g., time series) violate this and can yield biased convergence.

- **Discrete vs. continuous effects:**  
  Discrete distributions like binomial can produce visibly jagged histograms even for moderate \(n\), requiring smoothing or larger sample sizes.

- **Monte Carlo noise:**  
  Finite repetitions (e.g., 2000) may yield minor randomness-induced deviations in histograms and convergence plots.

### ğŸš€ Possible Extensions
- **Multivariate CLT:**  
  Extend the simulation to vector-valued variables, demonstrating how sample means converge jointly to a multivariate normal distribution.

- **Berryâ€“Esseen bounds:**  
  Quantify how fast convergence occurs by comparing empirical skewness and kurtosis with theoretical error bounds.

- **Bootstrap comparison:**  
  Compare sampling distributions from CLT to bootstrapped estimates of the mean â€” useful in small sample contexts.

- **Real-world data:**  
  Apply the sampling method to real datasets (e.g., income data, biological measures) to observe deviations from theoretical expectations.

- **Animated convergence:**  
  Generate an animated GIF or slider visualization showing histograms evolving with \(n\), to better visualize "normality emerging".

---

## 8 Â· Results & Interpretation ğŸ§©

- **Uniform[0,1]**  
  - At \(n=5\) the sampling distribution is already mildly bellâ€shaped.  
  - By \(n=30\)â€“50 the histogram and overlaid Normal(\(\mu=0.5,\;\sigma/\sqrt{n}\)) PDF are nearly indistinguishable.  
- **Exponential(Î»=1)**  
  - For \(n=5\), the mean distribution retains clear rightâ€skew.  
  - Convergence accelerates: \(n=30\) shows only slight skew, \(n=50\) closely follows the theoretical Normal(\(\mu=1,\;\sigma/\sqrt{n}\)).  
- **Binomial(n=10,p=0.3)**  
  - Discreteness visible at \(n=5\), but by \(n=10\) the â€œbarsâ€ align well with the continuous Normal(\(\mu=3,\;\sigma/\sqrt{n}\)).  
  - At \(n=30\)â€“50 the binomial sampling distribution is effectively Gaussian.  
- **Scale & spread**  
  - Empirical standard deviation of \(\bar X\) matches \(\sigma/\sqrt{n}\) within Monte Carlo error.  
- **Skew reduction**  
  - Observed skewness of \(\bar X\) decreases roughly as \(O(1/\sqrt{n})\), confirming theoretical rates.

---

<a name="conclusions"></a>
## 9 Â· Conclusions ğŸ“

1. **The CLT is robust**: regardless of original shape (uniform, skewed, or discrete), \(\bar X\) approaches normality.  
2. **Moderate \(n\)** (â‰ˆ30) suffices for practical normality in most casesâ€”heavily skewed distributions need larger \(n\).  
3. **Theoretical predictions** for mean and variance of \(\bar X\) hold quantitatively in simulation.  
4. **Simulations complement theory**, providing intuition about convergence speed and finiteâ€sample behavior.  
5. **Applications**: justifies use of confidence intervals and hypothesis tests even when data deviate from normality.

---

<a name="references"></a>
## 10 Â· Further Reading ğŸ“š

1. **W. Feller** â€” *An Introduction to Probability Theory and Its Applications*, Vol. 2, Ch. 8â€“10.  
   [Google Books](https://books.google.com/books/about/An_Introduction_to_Probability_Theory_an.html?id=1Y6IzgEACAAJ)

2. **L. Wasserman** â€” *All of Statistics: A Concise Course in Statistical Inference*, Â§ 2.3.  
   [Google Books](https://books.google.com/books/about/All_of_Statistics.html?id=th3fbFI1DaMC)

3. **G. Casella & R. L. Berger** â€” *Statistical Inference*, Â§ 7.1â€“7.2.  
   [Google Books](https://books.google.com/books/about/Statistical_Inference.html?id=ZpkPPwAACAAJ)

4. **D. S. Sivia & J. Skilling** â€” *Data Analysis: A Bayesian Tutorial*, Appendix on sampling distributions.  
   [Google Books](https://books.google.com/books/about/Data_Analysis.html?id=Kxx8CwAAQBAJ)

5. **SciPy Documentation** â€” [`scipy.stats.norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) (Normal distribution utilities).  
